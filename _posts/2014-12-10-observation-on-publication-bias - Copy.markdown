---
layout: post
title:  "Observation on publication bias"
date:   2014-12-10
categories: publication bias
---
Today, I committed a statistical crime: I killed a research idea without telling anyone. I've been thinking about the possible effect of changes in the BAC limit on inter-partner violence (IPV), also known as domestic violence. My mental model of IPV suggests two that the sign of the effect is theoretically ambiguous:

First, the BAC limit might induce a reduction in excessive alcohol consumption, which might in turn lead to a drop in IPV. This claim relies on the belief that alcohol consumption is a driver (in the causal sense) of IPV. I believe this, but the [literature](http://www.vawnet.org/applied-research-papers/print-document.php?doc_id=1324) is not settled on this point. On the other hand, a BAC limit might cause spouses to leave the bar and to continue their drinking at home, thus increasing the probability of IPV.

As an empirical economist, my preferred approach to resolving this is to take the question to the data. After about a day downloading, reformatting, and linking data, I had a dataset to run some preliminary regressions. The big moment! 

... but I didn't find anything. Well, I mostly didn't find anything. Mostly I found that there weren't enough states with good data on IPV at the time that the BAC limit laws were passed to generate the amount of power I needed to see a significant result, after appropriately controlling for omitted variables bias and clustering at the appropriate level. So, I shelved the project for now, which realistically means that I will likely never work on it again. 

As a graduate student whose future job prospects depend on my ability to produce interesting research in a short time frame, this makes sense. But as a scientist (or, more honestly, someone who is trying to do work that resembles science), this pains me since it's a perfect example of [publication bias](http://en.wikipedia.org/wiki/Publication_bias). I'm not attempting to publish these results because the cost of putting together a result is high and the likelihood that anyone would want to put it in a paper is very, very low. But by not doing so, I'm depriving the world of a bit of useful knowledge, tiny though at may be.

I am convinced that this is a near-daily occurance in social science. Thousands of graduate students and early-career professors churn through piles of data and specifications every day, looking for interesting and publishable results. And for the most part, they do it honestly: starting with some reasonable theory, not pushing the data and model choice for specifications that show them what they want, and stopping when it seems unreasonable to continue. A recent [paper in Science](http://www.sciencemag.org/content/345/6203/1502) documents evidence of exactly this kind of publication bias in the social sciences.

So what should we do with null results? It's unclear, and [controversial](http://www.sciencemag.org/content/345/6200/992.full). I personally support the idea of a null result registry, but suspect that unless the incentives for publishing results change drastically, few academics will take advantage of it. Time, after all, is a scarce resource.